{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import re\n",
    "\n",
    "resume_path = '/Users/sakshigupta/Desktop/FYP/AI-RecruitmentTool/back-end/data/sample.pdf'\n",
    "job_description = '/Users/sakshigupta/Desktop/FYP/AI-RecruitmentTool/back-end/data/job_description.txt'\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase and remove special characters\n",
    "    text = re.sub(r'[^\\w\\s]', '', text.lower())\n",
    "    return text\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        text = ''\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "def extract_data_from_text(file_path) :\n",
    "    file_contents =''\n",
    "    with open(file_path, 'r') as file:\n",
    "        file_contents = file.read()\n",
    "    return file_contents\n",
    "\n",
    "\n",
    "resume_text = extract_text_from_pdf(resume_path)\n",
    "job_description_text = extract_data_from_text(job_description)\n",
    "processed_resume = preprocess_text(resume_text)\n",
    "processed_job_description = preprocess_text(job_description_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "count_matrix=vectorizer.fit_transform([processed_resume,processed_job_description])\n",
    "MatchPercentage=cosine_similarity(count_matrix)[0][1]*100\n",
    "print(f'Match Percentage is: {MatchPercentage:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "count_matrix=vectorizer.fit_transform([processed_resume,processed_job_description])\n",
    "MatchPercentage=cosine_similarity(count_matrix)[0][1]*100\n",
    "print(f'Match Percentage is: {MatchPercentage:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# !!! Kernel crashing on local - added code in google collab\n",
    "# https://colab.research.google.com/drive/1kOPN6OPJqdJAif8ClZcQnUcsRBn2z_Ag?usp=sharing\n",
    "\n",
    "\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "# def sentence_bert_cosine_similarity(resume, job_description):\n",
    "#     model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "    \n",
    "#     # Ensure inputs are lists\n",
    "#     resumes = [resume] if isinstance(resume, str) else resume\n",
    "#     job_descriptions = [job_description] if isinstance(job_description, str) else job_description\n",
    "    \n",
    "#     document_embeddings = model.encode(resumes + job_descriptions)\n",
    "    \n",
    "#     resume_embeddings = document_embeddings[:len(resumes)]\n",
    "#     job_embeddings = document_embeddings[len(resumes):]\n",
    "    \n",
    "#     return cosine_similarity(resume_embeddings, job_embeddings)\n",
    "\n",
    "\n",
    "# sentence_bert_similarities = sentence_bert_cosine_similarity(processed_resume, processed_job_description)\n",
    "# print(f\"Sentence-BERT similarity: {sentence_bert_similarities[0][0]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import base64\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# MongoDB connection\n",
    "client = MongoClient('mongodb://localhost:27017/')\n",
    "db = client['AI-RECRUITMENT']\n",
    "collection = db['CANDIDATES']\n",
    "\n",
    "# Function to encode DOCX file to base64\n",
    "def encode_docx_to_base64(docx_path):\n",
    "    with open(docx_path, \"rb\") as docx_file:\n",
    "        encoded_string = base64.b64encode(docx_file.read()).decode('utf-8')\n",
    "    return encoded_string\n",
    "\n",
    "# Path to the directory containing the DOCX files\n",
    "docx_directory = '/Users/sakshigupta/Downloads/Resumes'\n",
    "\n",
    "# Iterate over all DOCX files in the directory\n",
    "for docx_file in os.listdir(docx_directory):\n",
    "    if docx_file.endswith('.docx'):\n",
    "        docx_path = os.path.join(docx_directory, docx_file)\n",
    "        candidate_name = os.path.splitext(docx_file)[0]\n",
    "        encoded_docx = encode_docx_to_base64(docx_path)\n",
    "        \n",
    "        # Prepare the document for MongoDB\n",
    "        document = {\n",
    "            \"name\": candidate_name,\n",
    "            \"resume\": encoded_docx,\n",
    "            \"cover_letter\": encoded_docx\n",
    "        }\n",
    "        \n",
    "        # Insert the document into MongoDB\n",
    "        collection.insert_one(document)\n",
    "        print(f\"Inserted {candidate_name}'s resume into MongoDB\")\n",
    "\n",
    "print(\"All DOCX files have been processed and inserted into MongoDB.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rough",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
